code:
    - extract additional features and create a new phi function.
    ? make the viterbi2 code more efficient.
    - cleanup the code.
    - write function documentation.

report:
    - descriptions of the functions we implemented.
    - short description of how we worked.
    - how we tested each function.
    - how we found the best parameters of the perceptron (rate, epoch, w0)
    V answer Q1 of the exercise.
    V answer Q2 of the exercise.
    - figures:
        X convergence of the perceptron w to the "ideal" w.
        - convergence of the ||w-w_prev|| between iterations.
        - convergence of ||W.mean() - W_prev.mean()||.
        - one of the above as a function of different ethas.
        - test loss as a function of iterations of the perceptron.
        - test loss as a function of the chosen model (HMM, HMM++) and the size of the training data.
        - test loss as a function of one extra feature we added and compare to regular HMM loss.


- is digit
- is upper-case
- length
- suffixes
- are there " before the word?
- special words before - a, is, and, the, it.
- is first word?
- is last word?
